# -*- coding: utf-8 -*-
"""
ä¸åŒ LLM åç«¯çš„å¯¹æ¯”ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Mockã€OpenAIã€é€šä¹‰åƒé—®ç­‰ä¸åŒçš„ LLM åç«¯ã€‚
"""
import sys
import argparse
import os

# è®¾ç½®UTF-8ç¼–ç ï¼ˆWindowså…¼å®¹ï¼‰
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

from philosofia import ask_philosophically


def test_mock_backend():
    """æµ‹è¯• Mock LLMï¼ˆæ— éœ€APIå¯†é’¥ï¼‰"""
    print("\n" + "=" * 70)
    print("Mock LLM åç«¯ï¼ˆæœ¬åœ°æ¨¡æ‹Ÿï¼Œæ— éœ€APIå¯†é’¥ï¼‰")
    print("=" * 70)

    question = "AIåº”è¯¥æ‹¥æœ‰æƒåˆ©å—ï¼Ÿ"
    print(f"\nğŸ“‹ é—®é¢˜: {question}\n")

    try:
        response = ask_philosophically(
            question,
            llm_backend="mock",
            use_llm=True,
        )
        print("ã€åˆé¢˜ã€‘")
        print(response["dialectical_synthesis"])
        print("\nã€æ¨ç†æ­¥éª¤æ•°ã€‘")
        print(f"{len(response.get('reasoning_chain', []))} æ­¥")
    except Exception as e:
        print(f"âŒ Mock åç«¯å¤±è´¥: {e}")


def test_openai_backend():
    """æµ‹è¯• OpenAI LLMï¼ˆéœ€è¦APIå¯†é’¥ï¼‰"""
    print("\n" + "=" * 70)
    print("OpenAI LLM åç«¯")
    print("=" * 70)

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸  æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡ï¼Œè·³è¿‡æ­¤æµ‹è¯•")
        return

    question = "AIåº”è¯¥æ‹¥æœ‰æƒåˆ©å—ï¼Ÿ"
    print(f"\nğŸ“‹ é—®é¢˜: {question}\n")
    print("æ€è€ƒä¸­ï¼ˆè°ƒç”¨OpenAI APIï¼‰...\n")

    try:
        response = ask_philosophically(
            question,
            llm_backend="openai",
            api_key=api_key,
            model="gpt-3.5-turbo",
            use_llm=True,
        )
        print("ã€åˆé¢˜ã€‘")
        print(response["dialectical_synthesis"][:300] + "...")
        print(f"\nã€æ¨¡å‹ã€‘OpenAI GPT-3.5-turbo")
    except Exception as e:
        print(f"âŒ OpenAI åç«¯å¤±è´¥: {e}")


def test_qwen_backend():
    """æµ‹è¯•é€šä¹‰åƒé—® LLMï¼ˆéœ€è¦APIå¯†é’¥ï¼‰"""
    print("\n" + "=" * 70)
    print("é€šä¹‰åƒé—® (Qwen) LLM åç«¯")
    print("=" * 70)

    api_key = os.getenv("QWEN_API_KEY")
    if not api_key:
        print("âš ï¸  æœªè®¾ç½® QWEN_API_KEY ç¯å¢ƒå˜é‡ï¼Œè·³è¿‡æ­¤æµ‹è¯•")
        return

    question = "å¦‚ä½•çœ‹å¾…AIä¼¦ç†ï¼Ÿ"
    print(f"\nğŸ“‹ é—®é¢˜: {question}\n")
    print("æ€è€ƒä¸­ï¼ˆè°ƒç”¨é€šä¹‰åƒé—®APIï¼‰...\n")

    try:
        response = ask_philosophically(
            question,
            llm_backend="qwen",
            api_key=api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            model="qwen-turbo",
            use_llm=True,
        )
        print("ã€åˆé¢˜ã€‘")
        print(response["dialectical_synthesis"][:300] + "...")
        print(f"\nã€æ¨¡å‹ã€‘é€šä¹‰åƒé—® (Qwen Turbo)")
    except Exception as e:
        print(f"âŒ é€šä¹‰åƒé—®åç«¯å¤±è´¥: {e}")


def test_no_llm():
    """æµ‹è¯•ä¸ä½¿ç”¨LLMçš„æœ¬åœ°æ¨¡å¼ï¼ˆä½¿ç”¨é¢„è®¾ç­”æ¡ˆï¼‰"""
    print("\n" + "=" * 70)
    print("æœ¬åœ°æ¨¡å¼ï¼ˆæ— LLMï¼Œä½¿ç”¨é¢„è®¾ç­”æ¡ˆï¼‰")
    print("=" * 70)

    question = "å¦‚ä½•å¹³è¡¡éšç§å’Œå®‰å…¨ï¼Ÿ"
    print(f"\nğŸ“‹ é—®é¢˜: {question}\n")

    try:
        response = ask_philosophically(
            question,
            use_llm=False,  # ä¸ä½¿ç”¨LLM
        )
        print("ã€å¤šè§†è§’ã€‘")
        for label, view in response["perspectives"].items():
            print(f"\n{label}:")
            print(f"  {view}")

        print("\nã€åˆé¢˜ã€‘")
        print(response["dialectical_synthesis"])
        print("\nâ±ï¸  å“åº”é€Ÿåº¦ï¼šæå¿«ï¼ˆæœ¬åœ°é¢„è®¾ï¼‰")
    except Exception as e:
        print(f"âŒ æœ¬åœ°æ¨¡å¼å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æµ‹è¯•ä¸åŒçš„ LLM åç«¯")
    parser.add_argument(
        "--backend",
        choices=["mock", "openai", "qwen", "local", "all"],
        default="all",
        help="è¦æµ‹è¯•çš„åç«¯",
    )
    args = parser.parse_args()

    print("=" * 70)
    print("Philosofia - LLM åç«¯å¯¹æ¯”æµ‹è¯•")
    print("=" * 70)

    if args.backend in ["mock", "all"]:
        test_mock_backend()

    if args.backend in ["openai", "all"]:
        test_openai_backend()

    if args.backend in ["qwen", "all"]:
        test_qwen_backend()

    if args.backend in ["local", "all"]:
        test_no_llm()

    print("\n" + "=" * 70)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
    print("=" * 70)


if __name__ == "__main__":
    main()
